---
title: Notes on a Randomized Saturation Design
author: Jake Bowers
date: '`r format(Sys.Date(), "%B %d, %Y")`'
bibliography: bibliography.bib
published: true
graphics: yes
fontsize: 10pt
geometry: margin=1in
mainfont: "Minion Pro"
output:
  html_document:
    graphics: yes
    fig_caption: yes
    fig_height: 4
    fig_width: 4
  pdf_document:
    latex_engine: xelatex
    graphics: yes
    fig_caption: yes
    fig_height: 4
    fig_width: 4
  word_document:
    fig_height: 3
    fig_width: 5

---


```{r include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.

## To make the html file do
## render("exploration4.Rmd",output_format=html_document(fig_retina=FALSE))
## To make the pdf file do
## render("exploration4.Rmd",output_format=pdf_document())

require(knitr)
opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small",    # slightly smaller font for code
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='footnotesize',
  out.width='.9\\textwidth',
  message=FALSE,
  comment=NA)
```



A randomized saturation design is like the simple two-level design that we describe in [twolevelrand.Rmd](twolevelrand.Rmd) but involves random assignment of proportion treated to counties, or random assignment of "saturation" of the treatment.^[We follow @baird2015designing, who in turn build on @sinclair2012detecting, among others.]  For example, we might randomly assign counties to receive between 0% treated (i.e. the "pure control" counties in the two-level design) and 100% treated, with some saturations in between (like 33%, 50%, 66%, etc.).

This design raises a couple of new questions for us.  If our total number of treatments is fixed at $n_t$, then, to maximize power and enhance interpretibility: How many saturation categories should we use? How many counties should we choose?

# Blocks

## State
We will block first on state because the agricultural outcomes and farmer behavior will vary by state. We expect, that the relationship between microloans applications, spillover, and other outcomes, might also vary by state (in part as a proxy for ecosystem/type of agriculture and in part as a proxy for other influences that states have on agricultural production). Also, since counties tend to have the same land area size within a state, but vary greatly in land area across states, we expect different patterns in spillover by state --- assigning treatment to 50% of the farmers in one large county might incur different spillover than 50% of the farmers in a smaller county.

## County Size (number of farmers)
If we are randomly assigning counties to saturation, we should also probably should assign saturation to sets of counties that are similar in number of farmers within them. Across many counties, the distribution of county size should be balanced across saturation treatments, in expectation, but in our finite sample, we think that we will enhance power and make comparisons easier to understand if, within states, we randomly assign saturation within blocks of similar counties (with number of farmers within the county being a key covariate, but perhaps there are others that matter as well).

## Other county characteristics that moderate treatment and/or predict outcomes?

# Saturation

Within blocks, we know that we want at least one county at saturation 0% (i.e the "pure control") condition.

# Power

We have two main effects: the ITT (whereby we compare treated to pure control individuals), the Spillover on the Non-Treated (SNT) (whereby we compare non-treated individuals in a county with some saturation $\pi>0$ to pure control individuals (people in counties with $\pi=0$).

The power of the design will depend on overall experimental pool $N$ (roughly 3 million); number treated $n_t$; the variation in the outcome, $Y$; the size of the direct effect of treatment; the size of the spillover effect; the intra-county dependence (measured by the intra-cluster correlation coefficient (ICC)); and the correlation between treatment status within counties. The Baird et al 2015 piece provides

## A Note on Inferential Frameworks

The Blair et al 2015 piece uses a random effects/model-based mode for statistical inference. They claim that this very much simplifies the work required for power analysis. Although, in general, I tend to advocate randomization inference for randomized experiments, I suspect that we might gain from following their lead here, especially since our total pool of units is so large --- that the concerns about consistency and the CLT that one sidesteps with randomization infeerence in its permutation form might not be that important. That said, we might still want to check our results using a more direct randomization inference approach if we have few saturation categories.


# What do our data look like?

The following reads the data from Google Drive, assuming that there is a Google Sheet called "county-counts.csv" in your sheets directory.

```{r getdatafromgoogle}
library(googlesheets)
countyCountsGS<-gs_title("countyCountsFIPS.csv") ## get info about the data
countyCounts<-as.data.frame(gs_read_csv(countyCountsGS,as.is=TRUE),stringsAsFactors=FALSE)
## countyCounts<-countyCounts[countyCounts$state!="PR",]
str(countyCounts)
countyCounts$STATEFIPS<-with(countyCounts,ifelse(state.fips<10,paste("0",state.fips,sep=""),as.character(state.fips)))
countyCounts$COUNTYNAME<-countyCounts$county
countyCounts$COUNTYFIPS<-with(countyCounts,ifelse(county.fips<10,
						  paste("00",county.fips,sep=""),
						  ifelse(county.fips<100,paste("0",county.fips,sep=""),
							 as.character(county.fips))))

countyCounts$combifips<-with(countyCounts,paste(STATEFIPS,COUNTYFIPS,sep=""))
str(countyCounts)
```

```{r descusdadat}
sapply(countyCounts,function(x){ sum(is.na(x) | x=="" ) })
with(countyCounts,tapply(n,state,sum)) ## N by state
with(countyCounts,tapply(county,state,function(x){ length(unique(x) ) })) ## number of counties by state
summary(with(countyCounts,tapply(n,county,sum))) ## number of farmers by county
sapply(split(countyCounts$n,countyCounts$state),function(x){ summary(x) }) ## dist of county sizes by state
```

Remove counties with fewer than 2 farmers (these seem to be duplicates in the file anyway):

```{r}
countyCounts<-countyCounts[countyCounts$n>2,]
names(table(countyCounts$combifips))[table(countyCounts$combifips)>1]
```

Anchorage is still duplicated, so removing by hand. Eventually the input file should only have one row per county.

```{r}
countyCounts<-countyCounts[!(countyCounts$n==4 & countyCounts$combifips=="02020"),]
stopifnot(all(table(countyCounts$combifips)==1))
```


#  County to County Distances

We can have three measures of distance between counties: euclidean distance
between geographic centers, euclidean distance between centers of population,
and an indicator (1 or 0) for whether two counties are adjacent to each other.^[
Some of the sources:
Centers of population <http://www2.census.gov/geo/docs/reference/cenpop2010/county/CenPop2010_Mean_CO.txt>
<https://www.census.gov/geo/reference/centersofpop.html>
County Adjacency:
<https://www.census.gov/geo/reference/county-adjacency.html>
]

## County Adjacency

This file required a bit of reformatting work. This next run once and by hand at the command line. The countyAdjClean.txt file will be in the repository.

```
##These are bash commands.
## They are currently in gis/setupAdjData.sh which can be run at the command line.
curl -O http://www2.census.gov/geo/docs/reference/county_adjacency.txt
cp county_adjacency.txt tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/"\t/";\t/g' tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/\t\t"/\t\t;;"/g' tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/\t"/\t;"/g' tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/\t//g' tmp1.txt
## Add "Watonwan County, MN" to line 9625
LC_CTYPE=C LANG=C sed -i.bak '9629s/.*/"Watonwan County, MN";27165;"Blue Earth County, MN";27013/' tmp1.txt
## Change the old FIPS code of 02195 for Petersburgh Alaska to the new one of 02280
LC_CTYPE=C LANG=C sed -i.bak 's/02195/02280/g' tmp1.txt
LC_CTYPE=C LANG=C tr  '\n' ':'  < tmp1.txt > tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/:;;/;/g'  tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/:/\\\n/g'  tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/ County//g' tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/ Census Area//g' tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/ and Borough//g' tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/ Borough//g' tmp2.txt
cp tmp2.txt countyAdjClean.txt
## change encoding of some non-English characters like Do√±a Ana County, NM, doing it by hand in vim:
### s/Do?a/Dona/gc
```


```{r readadjmat}
## https://stackoverflow.com/questions/5411979/state-name-to-abbreviation-in-r

Sys.setlocale('LC_ALL','C')

cntyAdjList1<-scan("gis/countyAdjClean.txt",what="",sep="\n",quote='\"')
cntyAdjList2<-strsplit(cntyAdjList1,";")
centerCounties1<-sapply(cntyAdjList2,function(obj){obj[1]})
centerCountiesFIPS1<-sapply(cntyAdjList2,function(obj){obj[2]})
centerCounties1[centerCounties1=="Prince of Wales-Hyder Census Area"]<-"Prince Wales Hyder"
names(cntyAdjList2)<-centerCounties1

## Remove Puerto Rico
centerCounties2<-centerCounties1[grep("PR$|VI$|GU$|AS$|MP$",centerCounties1,invert=TRUE)]
centerCountiesFIPS2<-centerCountiesFIPS1[grep("PR$|VI$|GU$|AS$|MP$",centerCounties1,invert=TRUE)]
cntyAdjList3<-cntyAdjList2[centerCounties2]

adjacentCounties<-lapply(cntyAdjList3,function(x){
			   if(length(x)<5){
			     return("None")
			   } else {
			     grep(",",x[3:length(x)],value=TRUE)
}})

isolatedCounties<-sapply(adjacentCounties,function(x){ all(x=="None") })
cntyAdjList3[isolatedCounties]

adjacentCountiesFIPS<-lapply(cntyAdjList3,function(x){ grep('[0-9]$',x[3:length(x)],value=TRUE) })
names(adjacentCountiesFIPS)<-centerCountiesFIPS2

```



## Centroids based on geography and based on population density.

```{r setupcensusfiles}
states<-read.csv("gis/StateNamesAndAbbreviations.csv",as.is=TRUE)

usaCountiesPopCentroids<-read.csv(url("http://www2.census.gov/geo/docs/reference/cenpop2010/county/CenPop2010_Mean_CO.txt"),as.is=TRUE)
usaCountiesPopCentroids$state<-states$Postal.Code[match(usaCountiesPopCentroids$STNAME,states$State)]
usaCountiesPopCentroids$STATEFIPS<-with(usaCountiesPopCentroids,ifelse(STATEFP<10,paste("0",STATEFP,sep=""),as.character(STATEFP)))
usaCountiesPopCentroids$COUNTYNAME<-usaCountiesPopCentroids$COUNAME

str(usaCountiesPopCentroids)
usaCountiesPopCentroids$COUNTYFIPS<-with(usaCountiesPopCentroids,ifelse(COUNTYFP<10,
									paste("00",COUNTYFP,sep=""),
									ifelse(COUNTYFP<100,paste("0",COUNTYFP,sep=""),
									       as.character(COUNTYFP))))


usaCountiesGeoCentroids<-read.dbf("gis/AddCombiFips/USACountyCentroids.dbf",as.is=TRUE)

tmp<-t(sapply(split(usaCountiesGeoCentroids,usaCountiesGeoCentroids$COMBIFIPS),function(dat){
				     sapply(dat,function(x){
					      if(is.character(x)){
						return(unique(x))
					      } else {
						return(mean(x))
					      }})}))

usaCountiesGeoCentroidsAvg<-as.data.frame(tmp,stringsAsFactors=FALSE)
usaCountiesGeoCentroidsAvg$CENTRDLON<-as.numeric(usaCountiesGeoCentroidsAvg$CENTRDLON)
usaCountiesGeoCentroidsAvg$CENTRDLAT<-as.numeric(usaCountiesGeoCentroidsAvg$CENTRDLAT)
str(usaCountiesGeoCentroidsAvg)

## Dumping repeated counties --- basically, multi-island counties with multiple geographic centroids
censusCounties<-merge(usaCountiesPopCentroids,usaCountiesGeoCentroidsAvg,all.x=TRUE,all.y=FALSE,by=c("COUNTYFIPS","STATEFIPS"))

## Clifton Forge, VA is a town not a county as of 2010, but only in usaCountiesGeo so not in this file
## censusCounties<-censusCounties[censusCounties$COUNTYNAME.x!="Clifton Forge",]

summary(censusCounties)

## There are a few places with population centroids but no geographic centroids. This is ok. I think that we'd prefer population centroids anyway.
blah<-censusCounties[is.na(censusCounties$CENTRDLAT),"COUNTYNAME.x"]
countyCounts$county[countyCounts$county %in% blah]
grep("Prince",countyCounts$county,value=TRUE)
censusCounties$COUNTYNAME.x[censusCounties$COUNTYNAME.x=="Prince of Wales-Hyder"]<-"Prince Wales Hyder"

## Remove Puerto Rico
censusCounties<-censusCounties[!(censusCounties$STNAME=="Puerto Rico" | censusCounties$state=="PR" | censusCounties$STATEFIPS=="72"),]
stopifnot(all(is.na(unlist(censusCounties[is.na(censusCounties$state),]))))
censusCounties<-censusCounties[!is.na(censusCounties$state),]

## Merge onto existing data.

censusCounties$county<-censusCounties$COUNTYNAME.x
usaCountiesPopCentroids$COUNTYNAME[usaCountiesPopCentroids$COUNTYNAME=="Prince of Wales-Hyder"]<-"Prince Wales Hyder"
usaCountiesPopCentroids$county<-usaCountiesPopCentroids$COUNTYNAME

tmpdat<-merge(countyCounts,censusCounties,by=c("county","state"),all.x=TRUE,all.y=FALSE)
##tmpdat<-merge(countyCounts,usaCountiesPopCentroids,by=c("county","state"),all.x=TRUE,all.y=FALSE)
tmpdat$ids<-paste(tmpdat$county,tmpdat$state,sep=", ")


## We have duplicated records because, I think, some citys and counties are both cities and counties.
## https://en.wikipedia.org/wiki/List_of_counties_in_Virginia
with(tmpdat,table(ids)[table(ids)!=1])
blah<-names(with(tmpdat,table(ids)[table(ids)!=1]))
blahdat<-tmpdat[tmpdat$ids %in% blah,]
blahdat<-blahdat[order(blahdat$county,blahdat$POPULATION,decreasing=TRUE),]
## The county FIPS codes are (from the TR-65 FIPS Code Chart)

bad<-with(tmpdat,{(county=="Richmond"  & state=="VA" & COUNTYFP!="159")  |
	          (county=="Bedford"   & state=="VA" & COUNTYFP!="019") |
	          (county=="Fairfax"   & state=="VA" & COUNTYFP!="059") |
	          (county=="Roanoke"   & state=="VA" & COUNTYFP!="161")|
	          (county=="Franklin"  & state=="VA" & COUNTYFP!="067") |
	          (county=="Baltimore" & state=="MD" & COUNTYFP!="005")})

tmpdat<-tmpdat[!bad,]
stopifnot(all(table(tmpdat$ids)==1))
row.names(tmpdat)<-tmpdat$ids

```

# Find pairs


Remove counties with only 1 farmer
```{r}
tmpdat<-tmpdat[tmpdat$n>1,]
```

Remove counties with no geographic information:
```{r}
tmpdat<-tmpdat[!is.na(tmpdat$LATITUDE),]
```

Also trim the top and bottom 1 percent of counties (in terms of size)

```{r}
stateCountyFarmDist<-tapply(tmpdat$n,tmpdat$state,function(x){ quantile(x,c(0,.01,.1,.25,.5,.75,.9,.99,1)) })

datList<-lapply(split(tmpdat,tmpdat$state),function(dat){
		  qs<-quantile(dat$n,c(.01,.99))
		  return(dat[dat$n >= qs[[1]] & dat$n <= qs[[2]],])
		  })

sapply(datList,nrow)
sum(sapply(datList,nrow))

wrkdat<-do.call("rbind",datList)
row.names(wrkdat)<-wrkdat$combifips

```

## Find Pairs

The input to the pairing function is a matrix of distances. So first we make distance matrices.

```{r setuppairs}
library(nbpMatching)

scalarDist<-function(var,scalefactor=1){
  ## Utility function to make n x n abs dist matrices
  ## Scalefactor helps us turn fractions into integers since nbpMatching needs
  ## integers
  outer(var,var,FUN=function(x,y){
    as.integer(abs(x-y)*scalefactor)
  })
}


## Make a matrix with 1 indicating that two counties are touching and 0 otherwise.
#### Not all counties in the adjacency matrix are represented in the working data from the USDA
#### blah<-sapply(centerCountiesFIPS2,function(x){
#### 	       all(adjacentCountiesFIPS[[x]] %in% wrkdat$combifips ) })
#### table(blah)

adjList<-lapply(wrkdat$combifips,function(x){
		  wrkdat$combifips %in% adjacentCountiesFIPS[[x]]
	       })
names(adjList)<-wrkdat$combifips

adjMat<-do.call("rbind",adjList)
stopifnot(all(diag(adjMat))) ## all diagonals should be true
adjMat<-adjMat*1 # convert to numbers
dimnames(adjMat)<-list(wrkdat$combifips,wrkdat$combifips)
diag(adjMat)<-0 ## set self-distances to zero for ease in penalty creation below
wrkdat[which(rowMeans(adjMat)==0 ),] ## which counties have to adjacent counties (Honolulu and Maui in Hawaii)

## Create distance matrix for differences in number of farmers
numfarmers<-wrkdat$n
names(numfarmers)<-row.names(wrkdat)
sizeDist<-scalarDist(numfarmers)

## Exclude delaware
wrkdat<-wrkdat[wrkdat$state!="DE",]

## Get the fips codes by state so that we can do state-by-state matching
fipsByState<-split(wrkdat$combifips,wrkdat$state)

blah<-sapply(fipsByState,function(fips){ isSymmetric(adjMat[fips,fips]) })

mnfips<-fipsByState[["MN"]]

for(i in 1:length(mnfips)){
mnList<-lapply(mnfips[-i],function(x){
		  mnfips[-i] %in% adjacentCountiesFIPS[[x]]
	       })
names(mnList)<-mnfips[-i]
mnMat<-do.call("rbind",mnList)
print(isSymmetric(mnMat))
}


getStatePairs<-function(fips){
  adjD<-adjMat[fips,fips]
  sizeD<-sizeDist[fips,fips]
  maxSizeDist<-max(as.vector(sizeD))
  penMat<- sizeD + adjD*maxSizeDist   ## add the maximum distance to any entry where two counties are adjacent
  if( (nrow(penMat) %% 2)!=0){
    penMat <- make.phantoms(penMat,1) ## delete one county where we have an odd number --- it will be the county least like the others, greatest distance on size and adjacency.
  }
  penMatD<-distancematrix(penMat)
  nbpm<-nonbimatch(penMatD)
  thepairs<-get.sets(nbpm$matches,remove.unpaired=TRUE)
  thepairsA<-as.character(thepairs) ## return as a character rather than a factor to make the state-by-state matching easier
  names(thepairsA)<-names(thepairs)
  return(thepairsA)
}


## This is inelegant but intelligible and not slow
pairsByState<-lapply(fipsByState[grep("MN",names(fipsByState),invert=TRUE)],function(fips){
		       getStatePairs(fips)})

## Because some states have an odd number of counties, one is excluded.
sum(sapply(pairsByState,length))
sapply(pairsByState,length)
## Compare to:
table(wrkdat$state)
sum(table(wrkdat$state))
nrow(wrkdat)

for(i in 1:length(pairsByState)){
  wrkdat[names(pairsByState[[i]]),"pm1"]<-pairsByState[[i]]
}

stopifnot(all(table(wrkdat$pm1)==2))
```

How well did the matching do? Here are descriptives of the pairs.

```{r}
pairedDat<-wrkdat[!is.na(wrkdat$pm1),]
pairDiffsN<-with(pairedDat,tapply(n,pm1,function(x){ abs(diff(x)) }) )
summary(pairDiffsN)
quantile(pairDiffsN,seq(0,1,.1))

```

## Choose best pair among the matches within each state restrict to not adjacent

```{r}

pairDiffsByState<-lapply(split(pairedDat,pairedDat$state),
		   function(dat){
		     with(dat,sort(tapply(n,pm1,function(x){ abs(diff(x)) })) )
		   })


```

## Assign one county to possible treatment within each pair.

## Calculate the number of treatment assignments.


# References
