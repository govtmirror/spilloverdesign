---
title: Notes on a Randomized Saturation Design
author: Jake Bowers
date: '`r format(Sys.Date(), "%B %d, %Y")`'
bibliography: bibliography.bib
published: true
graphics: yes
fontsize: 10pt
geometry: margin=1in
mainfont: "Minion Pro"
output:
  html_document:
    graphics: yes
    fig_caption: yes
    fig_height: 4
    fig_width: 4
  pdf_document:
    latex_engine: xelatex
    graphics: yes
    fig_caption: yes
    fig_height: 4
    fig_width: 4
  word_document:
    fig_height: 3
    fig_width: 5

---


```{r include=FALSE, cache=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.

## To make the html file do
## render("exploration4.Rmd",output_format=html_document(fig_retina=FALSE))
## To make the pdf file do
## render("exploration4.Rmd",output_format=pdf_document())

require(knitr)
opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small",    # slightly smaller font for code
  echo=TRUE,
  results='markup',
  strip.white=TRUE,
  fig.path='figs/fig',
  cache=FALSE,
  highlight=TRUE,
  width.cutoff=132,
  size='footnotesize',
  out.width='.9\\textwidth',
  message=FALSE,
  comment=NA)
```



A randomized saturation design is like the simple two-level design that we describe in [twolevelrand.Rmd](twolevelrand.Rmd) but involves random assignment of proportion treated to counties, or random assignment of "saturation" of the treatment.^[We follow @baird2015designing, who in turn build on @sinclair2012detecting, among others.]  For example, we might randomly assign counties to receive between 0% treated (i.e. the "pure control" counties in the two-level design) and 100% treated, with some saturations in between (like 33%, 50%, 66%, etc.).

This design raises a couple of new questions for us.  If our total number of treatments is fixed at $n_t$, then, to maximize power and enhance interpretibility: How many saturation categories should we use? How many counties should we choose?

# Blocks

## State
We will block first on state because the agricultural outcomes and farmer behavior will vary by state. We expect, that the relationship between microloans applications, spillover, and other outcomes, might also vary by state (in part as a proxy for ecosystem/type of agriculture and in part as a proxy for other influences that states have on agricultural production). Also, since counties tend to have the same land area size within a state, but vary greatly in land area across states, we expect different patterns in spillover by state --- assigning treatment to 50% of the farmers in one large county might incur different spillover than 50% of the farmers in a smaller county.

## County Size (number of farmers)
If we are randomly assigning counties to saturation, we should also probably should assign saturation to sets of counties that are similar in number of farmers within them. Across many counties, the distribution of county size should be balanced across saturation treatments, in expectation, but in our finite sample, we think that we will enhance power and make comparisons easier to understand if, within states, we randomly assign saturation within blocks of similar counties (with number of farmers within the county being a key covariate, but perhaps there are others that matter as well).

## Other county characteristics that moderate treatment and/or predict outcomes?

# Saturation 

Within blocks, we know that we want at least one county at saturation 0% (i.e the "pure control") condition.

# Power

We have two main effects: the ITT (whereby we compare treated to pure control individuals), the Spillover on the Non-Treated (SNT) (whereby we compare non-treated individuals in a county with some saturation $\pi>0$ to pure control individuals (people in counties with $\pi=0$).

The power of the design will depend on overall experimental pool $N$ (roughly 3 million); number treated $n_t$; the variation in the outcome, $Y$; the size of the direct effect of treatment; the size of the spillover effect; the intra-county dependence (measured by the intra-cluster correlation coefficient (ICC)); and the correlation between treatment status within counties. The Baird et al 2015 piece provides 

## A Note on Inferential Frameworks

The Blair et al 2015 piece uses a random effects/model-based mode for statistical inference. They claim that this very much simplifies the work required for power analysis. Although, in general, I tend to advocate randomization inference for randomized experiments, I suspect that we might gain from following their lead here, especially since our total pool of units is so large --- that the concerns about consistency and the CLT that one sidesteps with randomization infeerence in its permutation form might not be that important. That said, we might still want to check our results using a more direct randomization inference approach if we have few saturation categories.


# What do our data look like?

The following reads the data from Google Drive, assuming that there is a Google Sheet called "county-counts.csv" in your sheets directory.

```{r}
library(googlesheets)
countyCountsGS<-gs_title("county-counts.csv") ## get info about the data
countyCounts<-gs_read_csv(countyCountsGS)
str(countyCounts)
countyCounts<-as.data.frame(countyCounts,stringsAsFactors=FALSE)
```

```{r}
sapply(countyCounts,function(x){ sum(is.na(x) | x=="" ) })
with(countyCounts,tapply(n,state,sum)) ## N by state
with(countyCounts,tapply(county,state,function(x){ length(unique(x) ) })) ## number of counties by state
summary(with(countyCounts,tapply(n,county,sum))) ## number of farmers by county
sapply(split(countyCounts$n,countyCounts$state),function(x){ summary(x) }) ## dist of county sizes by state
```


#  County to County Distances

We can have three measures of distance between counties: euclidean distance
between geographic centers, euclidean distance between centers of population,
and an indicator (1 or 0) for whether two counties are adjacent to each other.^[
Some of the sources:
Centers of population <http://www2.census.gov/geo/docs/reference/cenpop2010/county/CenPop2010_Mean_CO.txt>
<https://www.census.gov/geo/reference/centersofpop.html>
County Adjacency:
<https://www.census.gov/geo/reference/county-adjacency.html>
]

## County Adjacency

This file required a bit of reformatting work.

```
##These are bash commands
curl -O http://www2.census.gov/geo/docs/reference/county_adjacency.txt
## Add "Watonwan County, MN" to line 9625
## This next doesn't work. doing it by hand
##sed -i.bak '9629s/.*/\"Watonwan County\, MN\"\t27165\t\"Blue Earth County\, MN\"\t27013' county_adjacency.txt
cp county_adjacency.txt tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/"\t/";\t/g' tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/\t\t"/\t\t;;"/g' tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/\t"/\t;"/g' tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/\t//g' tmp1.txt
LC_CTYPE=C LANG=C tr  '\n' ':'  < tmp1.txt > tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/:;;/;/g'  tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/:/\\\n/g'  tmp2.txt
cp tmp2.txt countyAdjClean.txt
## change encoding of some non-English characters like Do√±a Ana County, NM, doing it by hand in vim:
### s/Do?a/Dona/gc
```


```{r}
## https://stackoverflow.com/questions/5411979/state-name-to-abbreviation-in-r

Sys.setlocale('LC_ALL','C') 

cntyAdjList1<-scan("gis/countyAdjClean.txt",what="",sep="\n",quote='\"')
cntyAdjList2<-strsplit(cntyAdjList1,";")
centerCounties<-sapply(cntyAdjList2,function(obj){obj[1]})
centerCounties[centerCounties=="Prince of Wales-Hyder Census Area"]<-"Prince Wales Hyder"

names(cntyAdjList2)<-centerCounties

## Remove Puerto Rico
centerCounties<-centerCounties[grep(", PR$",centerCounties,invert=TRUE)]
adjacentCounties<-lapply(cntyAdjList2,function(x){ x[5:length(x)] })
```



## Centroids based on geography and based on population density.

```{r}
states<-read.csv("gis/StateNamesAndAbbreviations.csv",as.is=TRUE)

usaCountiesPopCentroids<-read.csv(url("http://www2.census.gov/geo/docs/reference/cenpop2010/county/CenPop2010_Mean_CO.txt"),as.is=TRUE)
usaCountiesPopCentroids$state<-states$Postal.Code[match(usaCountiesPopCentroids$STNAME,states$State)]
usaCountiesPopCentroids$STATEFIPS<-with(usaCountiesPopCentroids,ifelse(STATEFP<10,paste("0",STATEFP,sep=""),as.character(STATEFP)))
usaCountiesPopCentroids$COUNTYNAME<-usaCountiesPopCentroids$COUNAME

str(usaCountiesPopCentroids)
usaCountiesPopCentroids$COUNTYFIPS<-with(usaCountiesPopCentroids,ifelse(COUNTYFP<10,
									paste("00",COUNTYFP,sep=""),
									ifelse(COUNTYFP<100,paste("0",COUNTYFP,sep=""),
									       as.character(COUNTYFP))))


usaCountiesGeoCentroids<-read.dbf("gis/AddCombiFips/USACountyCentroids.dbf",as.is=TRUE)

tmp<-t(sapply(split(usaCountiesGeoCentroids,usaCountiesGeoCentroids$COMBIFIPS),function(dat){
				     sapply(dat,function(x){
					      if(is.character(x)){ 
						return(unique(x))
					      } else {
						return(mean(x))
					      }})}))

usaCountiesGeoCentroidsAvg<-as.data.frame(tmp,stringsAsFactors=FALSE)
usaCountiesGeoCentroidsAvg$CENTRDLON<-as.numeric(usaCountiesGeoCentroidsAvg$CENTRDLON)
usaCountiesGeoCentroidsAvg$CENTRDLAT<-as.numeric(usaCountiesGeoCentroidsAvg$CENTRDLAT)
str(usaCountiesGeoCentroidsAvg)

## Dumping repeated counties --- basically, multi-island counties with multiple geographic centroids
censusCounties<-merge(usaCountiesPopCentroids,usaCountiesGeoCentroidsAvg,all.x=TRUE,all.y=FALSE,by=c("COUNTYFIPS","STATEFIPS"))

## Clifton Forge, VA is a town not a county as of 2010, but only in usaCountiesGeo so not in this file
## censusCounties<-censusCounties[censusCounties$COUNTYNAME.x!="Clifton Forge",]

summary(censusCounties)

## There are a few places with population centroids but no geographic centroids. This is ok. I think that we'd prefer population centroids anyway.
blah<-censusCounties[is.na(censusCounties$CENTRDLAT),"COUNTYNAME.x"]
countyCounts$county[countyCounts$county %in% blah]
grep("Prince",countyCounts$county,value=TRUE)
censusCounties$COUNTYNAME.x[censusCounties$COUNTYNAME.x=="Prince of Wales-Hyder"]<-"Prince Wales Hyder"

## Remove Puerto Rico
censusCounties<-censusCounties[!(censusCounties$STNAME=="Puerto Rico" | censusCounties$state=="PR" | censusCounties$STATEFIPS=="72"),]
stopifnot(all(is.na(unlist(censusCounties[is.na(censusCounties$state),]))))
censusCounties<-censusCounties[!is.na(censusCounties$state),]

## Merge onto existing data.

censusCounties$county<-censusCounties$COUNTYNAME.x
usaCountiesPopCentroids$COUNTYNAME[usaCountiesPopCentroids$COUNTYNAME=="Prince of Wales-Hyder"]<-"Prince Wales Hyder"
usaCountiesPopCentroids$county<-usaCountiesPopCentroids$COUNTYNAME

tmpdat<-merge(countyCounts,censusCounties,by=c("county","state"),all.x=TRUE,all.y=FALSE)
##tmpdat<-merge(countyCounts,usaCountiesPopCentroids,by=c("county","state"),all.x=TRUE,all.y=FALSE)
tmpdat$ids<-paste(tmpdat$county,tmpdat$state,sep=", ")
with(tmpdat,table(ids)[table(ids)!=1])
blah<-names(with(tmpdat,table(ids)[table(ids)!=1]))
blahdat<-tmpdat[tmpdat$id %in% blah,]
blahdat<-blahdat[order(blahdat$county,blahdat$POPULATION,decreasing=TRUE),]


## We have duplicated records because, I think, some citys and counties are both cities and counties.
## https://en.wikipedia.org/wiki/List_of_counties_in_Virginia

## Which one to use? Which are referred to in the USDA data?

## The county FIPS codes are (from the TR-65 FIPS Code Chart)

bad<-with(tmpdat,{(county=="Richmond"   & state=="VA" &   COUNTYFIPS!="159")  |
	          (county=="Bedford"   & state=="VA" &   COUNTYFIPS!="019") |
	          (county=="Fairfax"   & state=="VA" &   COUNTYFIPS!="059") |
	          (county=="Roanoke"   & state=="VA" &   COUNTYFIPS!="161")|
	          (county=="Franklin"  & state=="VA" &  COUNTYFIPS!="067") |
	          (county=="Baltimore" & state=="MD" & COUNTYFIPS!="005")})

tmpdat<-tmpdat[!bad,]
stopifnot(all(table(tmpdat$ids)==1))
row.names(tmpdat)<-tmpdat$ids

```

# Find pairs


Remove counties with only 1 farmer
```{r}
tmpdat<-tmpdat[tmpdat$n>1,]
```

Remove counties with no geographic information:
```{r}
tmpdat<-tmpdat[!is.na(tmpdat$LATITUDE),]
```

Also trim the top and bottom 1 percent of counties (in terms of size)

```{r}
stateCountyFarmDist<-tapply(tmpdat$n,tmpdat$state,function(x){ quantile(x,c(0,.01,.1,.25,.5,.75,.9,.99,1)) })

datList<-lapply(split(tmpdat,tmpdat$state),function(dat){
		  qs<-quantile(dat$n,c(.01,.99))
		  return(dat[dat$n >= qs[[1]] & dat$n <= qs[[2]],])
		  })

sapply(datList,nrow)
sum(sapply(datList,nrow))

wrkdat<-do.call("rbind",datList)

```

## Find Pairs

The input to the pairing function is a matrix of distances. So first we make distance matrices.

```{r}
library(nbpMatching)

scalarDist<-function(var,scalefactor=1){
  ## Utility function to make n x n abs dist matrices
  ## Scalefactor helps us turn fractions into integers since nbpMatching needs
  ## integers
  outer(var,var,FUN=function(x,y){
    as.integer(abs(x-y)*scalefactor)
  })
}

## Matrix recording whether two counties are adjacent.




datByState<-split(wrkdat,wrkdat$state)

## This is inelegant but intelligible and not slow
pairsByState<-lapply(split(wrkdat,wrkdat$state), function(dat){
		       ## message(unique(dat$state))
		       dat<-datByState[[2]]
		       numfarmers<-dat$n
		       names(numfarmers)<-row.names(dat)
		       sizeDist<-scalarDist(numfarmers)
		       popCentroidDist<-as.matrix(dist(dat[,c("LATITUDE","LONGITUDE")]))
		       tmp<- 1 - popCentroidDist/max(as.vector(popCentroidDist))
		       diag(tmp)<-0
		       pendist<- sizeDist + tmp*sizeDist
		       nbpm<-nonbimatch(distancematrix(pendist))
		       thepairs<-get.sets(nbpm$matches,remove.unpaired=TRUE)
		       levels(thepairs)<-paste(unique(dat$state),levels(thepairs),sep=":")
		       thepairsA<-as.character(thepairs)
		       names(thepairsA)<-names(thepairs)
		       return(thepairsA)
  })

## Because some states have an odd number of counties, one is excluded.
sum(sapply(pairsByState,length))
sapply(pairsByState,length)
## Compare to:
table(wrkdat$state)
sum(table(wrkdat$state))
nrow(wrkdat)

for(i in 1:length(pairsByState)){
  message(i)
  wrkdat[names(pairsByState[[i]]),"pm1"]<-pairsByState[[i]]
}

stopifnot(all(table(wrkdat$pm1)==2))
```

How well did the matching do? Here are descriptives of the pairs.

```{r}
pairDiffsN<-with(wrkdat[!is.na(wrkdat$pm1),],tapply(n,pm1,function(x){ abs(diff(x)) }) )
summary(pairDiffsN)
quantile(pairDiffsN,seq(0,1,.1))

```

```{r}
## Maybe use a gist or something for this
source(url("http://www.jakebowers.org/Matching/nonbimatchingfunctions.R"))

```
## Choose best pair among the matches within each state

## Assign one county to possible treatment

## Calculate the number of treatment assignments 


# References
